{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "## Building a GPT\n",
        "\n",
        "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT. \n",
        "\n",
        "Modifications:\n",
        "input is Don Quixote \n",
        "Tokenizer is sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('don-quixote.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "ed819dd0-72e5-40a6-d2ed-928ff73bfda6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters:  1714083\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "25ca7adc-b8c0-42d1-b08c-e0863c5c314e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VOLUME I.\n",
            "\n",
            "\n",
            "CHAPTER I.\n",
            "\n",
            "WHICH TREATS OF THE CHARACTER AND PURSUITS OF THE FAMOUS GENTLEMAN DON\n",
            "QUIXOTE OF LA MANCHA\n",
            "\n",
            "\n",
            "In a village of La Mancha, the name of which I have no desire to call to\n",
            "mind, there lived not long since one of those gentlemen that keep a lance\n",
            "in the lance-rack, an old buckler, a lean hack, and a greyhound for\n",
            "coursing. An olla of rather more beef than mutton, a salad on most\n",
            "nights, scraps on Saturdays, lentils on Fridays, and a pigeon or so extra\n",
            "on Sundays, made away with three-quarters of his income. The rest of it\n",
            "went in a doublet of fine cloth and velvet breeches and shoes to match\n",
            "for holidays, while on week-days he made a brave figure in his best\n",
            "homespun. He had in his house a housekeeper past forty, a niece under\n",
            "twenty, and a lad for the field and market-place, who used to saddle the\n",
            "hack as well as handle the bill-hook. The age of this gentleman of ours\n",
            "was bordering on fifty; he was of a hardy habit, spare, gaunt-featured, a\n",
            "very early riser and a gre\n"
          ]
        }
      ],
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " !\"'(),-.01246:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "71\n"
          ]
        }
      ],
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text))) #list of set of all char in text\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "using SentencePiece instead of character-level tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input = \"don-quixote.txt\",\n",
        "    model_prefix = \"quixote\",\n",
        "    vocab_size = 4000, #subject to change i think ill try change later\n",
        "    character_coverage = 1.0, #all chars\n",
        "    pad_id = 3,\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
      },
      "outputs": [],
      "source": [
        "sp = spm.SentencePieceProcessor(model_file=\"quixote.model\")\n",
        "\n",
        "vocab_size = sp.get_piece_size()\n",
        "\n",
        "encode = lambda s: sp.encode(s, out_type = int)\n",
        "decode = lambda l: sp.decode(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[14, 187, 101, 634, 209]\n",
            "hello fellas\n"
          ]
        }
      ],
      "source": [
        "test = \"hello fellas\"\n",
        "print(encode(test))\n",
        "print(decode(encode(test)))\n",
        "#yusss its working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 4000\n",
            "\n",
            "Sample vocabulary pieces:\n",
            "  Token 10: \"▁that\"\n",
            "  Token 11: \"▁a\"\n",
            "  Token 12: \"▁in\"\n",
            "  Token 13: \"▁I\"\n",
            "  Token 14: \"▁he\"\n",
            "  Token 15: \";\"\n",
            "  Token 16: \"▁it\"\n",
            "  Token 17: \"▁\"\"\n",
            "  Token 18: \".\"\n",
            "  Token 19: \"▁for\"\n",
            "  Token 20: \"▁his\"\n",
            "  Token 21: \"▁as\"\n",
            "  Token 22: \"ed\"\n",
            "  Token 23: \"▁be\"\n",
            "  Token 24: \"▁not\"\n",
            "  Token 25: \"▁is\"\n",
            "  Token 26: \"▁was\"\n",
            "  Token 27: \"▁him\"\n",
            "  Token 28: \"ing\"\n",
            "  Token 29: \"▁with\"\n"
          ]
        }
      ],
      "source": [
        "print(f'Vocab size: {vocab_size}')\n",
        "\n",
        "#see what some tokens look like\n",
        "print('\\nSample vocabulary pieces:')\n",
        "for i in range(10, 30):\n",
        "    print(f'  Token {i}: \"{sp.id_to_piece(i)}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape: torch.Size([425906])\n",
            "dtype: torch.int64\n",
            "first 20 tokens: tensor([1078,  266,  503,  461, 2299,   13,   18,  299,   13,   18,  547, 2097,\n",
            "         194,  214,  163,   45, 2208,  248,  341,  248])\n",
            "decoded back: \"VOLUME I. CHAPTER I. WHICH TREATS OF THE CHARA\"\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "print(f\"Data shape: {data.shape}\") #shakespear was 1.1M  \n",
        "print(f\"dtype: {data.dtype}\")\n",
        "print(f\"first 20 tokens: {data[:20]}\")\n",
        "print(f'decoded back: \"{decode(data[:20].tolist())}\"')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train tokens: 383315 val tokens: 42591\n"
          ]
        }
      ],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(f\"train tokens: {len(train_data)} val tokens: {len(val_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first block_size+1 tokens: tensor([1078,  266,  503,  461, 2299,   13,   18,  299,   13,   18,  547, 2097,\n",
            "         194,  214,  163,   45, 2208,  248,  341,  248,  600,  252,  995,  466,\n",
            "         308,  461,  341,  194,  461,   76,  252,  194,  214,  163,  559, 3444,\n",
            "         194, 3657,  550,  565,  214, 2185, 2601,  393,   11,  395,    8,  505,\n",
            "         630,    4,    5,  221,    8,   52,   13,   34,   71,  465,    7,  364,\n",
            "           7,  263,    4,   72, 1909])\n",
            "decoded: \"VOLUME I. CHAPTER I. WHICH TREATS OF THE CHARACTER AND PURSUITS OF THE FAMOUS GENTLEMAN DON QUIXOTE OF LA MANCHA In a village of La Mancha, the name of which I have no desire to call to mind, there lived\"\n"
          ]
        }
      ],
      "source": [
        "#of uses smaller block size but these tokens carry moremeaning so ill use larger blocks \n",
        "block_size = 64\n",
        "print(f\"first block_size+1 tokens: {train_data[:block_size+1]}\")\n",
        "print(f'decoded: \"{decode(train_data[:block_size+1].tolist())}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input: [1078] (\"V\") -> target: 266 (\"O\")\n",
            "input: [1078, 266] (\"VO\") -> target: 503 (\"L\")\n",
            "input: [1078, 266, 503] (\"VOL\") -> target: 461 (\"U\")\n",
            "input: [1078, 266, 503, 461] (\"VOLU\") -> target: 2299 (\"ME\")\n",
            "input: [1078, 266, 503, 461, 2299] (\"VOLUME\") -> target: 13 (\"I\")\n",
            "input: [1078, 266, 503, 461, 2299, 13] (\"VOLUME I\") -> target: 18 (\".\")\n",
            "input: [1078, 266, 503, 461, 2299, 13, 18] (\"VOLUME I.\") -> target: 299 (\"CHAPTER\")\n",
            "input: [1078, 266, 503, 461, 2299, 13, 18, 299] (\"VOLUME I. CHAPTER\") -> target: 13 (\"I\")\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(min(8, block_size)):  #first 8 example\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f'input: {context.tolist()} (\"{decode(context.tolist())}\") -> target: {target.item()} (\"{decode([target.item()])}\")')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs shape:  torch.Size([4, 64])\n",
            "targets shape: torch.Size([4, 64])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "\n",
        "def get_batch(split):\n",
        "    d = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(d) - block_size, (batch_size,))\n",
        "    x = torch.stack([d[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([d[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print(f\"inputs shape:  {xb.shape}\")\n",
        "print(f\"targets shape: {yb.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "a650f8dc-da81-400b-bc59-0a595487fdb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 113,    7, 2650, 1449,   80,  181,  482,   67,    7,  136, 1823,    4,\n",
            "           19,    5, 1327,   25,   21,  578,  182, 2890,   12,  922,    4,    6,\n",
            "          374,   32,    9,   97, 1274,  631,  381,   37,    6,    5, 3608,   22,\n",
            "          109,    4,    6, 2871,  509, 2170,   96,   12,  445,  291,   18,  599,\n",
            "          424, 1969,  758,   73,  415,  464,  998,   30,  887, 3012,   46,   12,\n",
            "          286,    4,  569,    6],\n",
            "        [ 266,  855,  194,  282,   76, 3992,  252,  589,   57,  700,   76, 1792,\n",
            "          466, 2576, 1920,  194,  601, 1703,  598,  385,   76,  194,  728, 1111,\n",
            "          194, 2935,  559, 1103,  163,  369, 1742,  248,  341,  369,  266,  779,\n",
            "          728,  840,  555, 1111,  194,   76, 1499,  214, 2220, 2821,  299,  526,\n",
            "         3992, 3992,   76,  214,  514, 1504,   45, 2936, 2503,  550,  565,  466,\n",
            "         1178, 3057, 1226, 2210],\n",
            "        [  20, 1870,    6, 2944,    7,   20,   92, 2962,   15,    5,  787,   75,\n",
            "           14,  371,    7,   13,  102,   24,   60,    4,  837,   16,   23,    5,\n",
            "         2969,    8, 1078,  425,  727, 1168,    6,    5, 1762,    8,  510,  212,\n",
            "          425,  186,   18, 1385,  986,   68,   37,  545,   32,    9,   45,  882,\n",
            "            9,    9,  144,    9,    4,  963,   67,   12, 2920,    4,   59,   36,\n",
            "         1100,   77,   10,   62],\n",
            "        [ 710,  189,    4,   52,   16, 1484,    9,   74,   79,  207,    6, 1369,\n",
            "           18,  105, 1284,    6, 3247,  683,   25,   89, 3938,    4,    6,  710,\n",
            "           98,    6, 1320,   98,   79, 2624,   25,    5,  573,    8, 1964,   15,\n",
            "            6,   14,   54, 1627,   40,   24,    7, 1624,   16,    4,   59,    7,\n",
            "          353,    6, 2136,   16,    4,  142, 1888,   11,  616, 1246,   64,   10,\n",
            "         1707,   22,   29,    5]])\n"
          ]
        }
      ],
      "source": [
        "print(xb) # our input to the transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logits shape: torch.Size([256, 4000])\n",
            "loss: 8.814183235168457\n",
            "Expected initial loss: 8.29\n",
            " ⁇ witted quietly She neither Quintanona historian wouldst strip overwhelmed lance into carter always fifteen interest bearing Andalusia plaza victory imaginedry entrance dream frightened teach graciousguish body guest quietly madness memor farmer cheer dainty be attire angry France plight worth shepherds ugly weary placeate bone shepherdessen queenwhich concluded robb esquire montera more them eyebrows WAS travelling cousin fault duchessardut miracle LETTER station surname absent imagination caveers entertain plac sense dozen Master abundance grow Saragossa black bigches L ta doubten rash writing straight explaincatime asked presented describe misery slash\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\"\"\"\n",
        "same bigram model as the tutorial only change is vocab_size is now 4000 instead of 65. \n",
        "This means the embedding table is much larger.\n",
        "\"\"\"\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(f\"logits shape: {logits.shape}\")\n",
        "print(f\"loss: {loss}\")\n",
        "#expected initial loss ≈ -ln(1/4000) 8.3 ish\n",
        "print(f\"Expected initial loss: {-torch.log(torch.tensor(1.0/vocab_size)):.2f}\")\n",
        "\n",
        "print(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss after x steps7.879528999328613\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(1000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f\"loss after x steps{loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ⁇ red dash rateForagnificence time spur have ke matchGood A exist finep HISTORY boobyCH laws lank ermine fallen sharp truth uncover fairThe shape terms defiance still derive cost wrote evident tempted withdrew fountain clothesot almost haste grievance prudent suspended credit carriesng slave French coast whole sorrow establishdependent lift send adroit circuit withdraw Pasamonte As girths wisdom Rodrigo slash stickTellter folly leave companion sought yard looking torture penance quality fault why feed conversation requisite conscience firmly before usual dear cleverINGk passion bare alarm very DULCINEA once E Montalvan devotion persuasion figure capital BEF Ragged follow trumpet covered manifest persecute misfortunewparent weaksoever bl ceremonyaz window witness reap outcryBe mother filled with amazement GENTLEMAN rest cleav knock Roland verily persuasion beg signature M twentyendWho together burned wickedand Camilla mishap lionSay he bad opposite inquir distant correct prepared Mirrors month \" right terms but grind cloak sort often rabble palfrey recognise peerless admit along inclination befallen M tranquil( mat appetitero arrived sever none strive aughten arrival approbation store reliev is ex white enchantment enormous leanByque Heaven Carpio\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=200)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XinV8nmAnmKN"
      },
      "source": [
        "## The mathematical trick in self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukiH-NbRBhA",
        "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a=\n",
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[ 2.,  7.],\n",
            "        [ 8., 11.],\n",
            "        [14., 16.]])\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "take away is \"reading\" past information\n",
        "\n",
        "token 1: [1, 0, 0] only sees itself \n",
        "token 2: [1, 1, 0] one previous token and itself \n",
        "token 3: [1, 1, 1] all three tokens before it\n",
        "\n",
        "then what if the model can learn which previous token matter more so R3 might look like\n",
        "[0.2, 0.7, 0.2].\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn(B, T, C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
      },
      "outputs": [],
      "source": [
        "#we want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n",
        "\n",
        "#x bag of words "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "averaging previous tokens as context "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril==0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "using -inf instead of 0 for masked positons. -inf become 0 after softmax (e^-inf = 0) and the ones become euqual shares. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril==0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x \n",
        "torch.allclose(xbow,xbow3)\n",
        "wei"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1hdtzXCjgL",
        "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5CvobiQ0pLr"
      },
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "outputs": [],
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) #gets too peaky, converges to one-hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$ \\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
        "\n",
        "dk is headsize which is a normalization so wei variance is preserved. otherwise softmax will converge to one-hot vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "7720fa58-0478-4e8a-86a7-502d4cce9443"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9cK9BoXCYb",
        "outputId": "6368ece0-600e-417d-8a91-7c1e5d750ba8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
